{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2695fce7",
   "metadata": {},
   "source": [
    "## This demo assume you are using the NeMo24:07 container\n",
    "# Run setup script to download fundation model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd20580-ab24-455f-8553-dcb8992a8342",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no change     /home/ldu/anaconda3/condabin/conda\n",
      "no change     /home/ldu/anaconda3/bin/conda\n",
      "no change     /home/ldu/anaconda3/bin/conda-env\n",
      "no change     /home/ldu/anaconda3/bin/activate\n",
      "no change     /home/ldu/anaconda3/bin/deactivate\n",
      "no change     /home/ldu/anaconda3/etc/profile.d/conda.sh\n",
      "no change     /home/ldu/anaconda3/etc/fish/conf.d/conda.fish\n",
      "no change     /home/ldu/anaconda3/shell/condabin/Conda.psm1\n",
      "no change     /home/ldu/anaconda3/shell/condabin/conda-hook.ps1\n",
      "no change     /home/ldu/anaconda3/lib/python3.12/site-packages/xontrib/conda.xsh\n",
      "no change     /home/ldu/anaconda3/etc/profile.d/conda.csh\n",
      "no change     /home/ldu/.bashrc\n",
      "No action taken.\n",
      "WARNING: A conda environment already exists at '/home/ldu/anaconda3/envs/sft'\n",
      "Remove existing environment (y/[n])? ^C\n",
      "\n",
      "CondaSystemExit: \n",
      "Operation aborted.  Exiting.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash code/setup_bare_metal.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7bded7-634a-4f7d-8c1f-a826493890bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# run sft script to download data, split data, run sft and test on fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d56273-f113-4340-a118-75e7c5234417",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert nemo model from hf format to nemo format, this will take a while...\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo I 2024-10-16 20:08:03 convert_mistral_7b_hf_to_nemo:148] loading checkpoint ./mistral-7B-hf/\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.16s/it]\n",
      "[NeMo I 2024-10-16 20:08:05 convert_mistral_7b_hf_to_nemo:152] loaded checkpoint ./mistral-7B-hf/\n",
      "[NeMo W 2024-10-16 20:08:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "[NeMo I 2024-10-16 20:08:05 convert_mistral_7b_hf_to_nemo:194] nemo_config: {'mcore_gpt': True, 'micro_batch_size': 4, 'global_batch_size': 8, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'encoder_seq_length': 4096, 'max_position_embeddings': 32768, 'num_layers': 32, 'hidden_size': 4096, 'ffn_hidden_size': 14336, 'num_attention_heads': 32, 'init_method_std': 0.02, 'use_scaled_init_method': True, 'hidden_dropout': 0.0, 'attention_dropout': 0.0, 'ffn_dropout': 0.0, 'kv_channels': None, 'apply_query_key_layer_scaling': True, 'normalization': 'rmsnorm', 'layernorm_epsilon': 1e-05, 'do_layer_norm_weight_decay': False, 'make_vocab_size_divisible_by': 128, 'pre_process': True, 'post_process': True, 'persist_layer_norm': True, 'bias': False, 'activation': 'fast-swiglu', 'headscale': False, 'transformer_block_type': 'pre_ln', 'openai_gelu': False, 'normalize_attention_scores': True, 'position_embedding_type': 'rope', 'rotary_percentage': 1.0, 'attention_type': 'multihead', 'share_embeddings_and_output_weights': False, 'overlap_p2p_comm': False, 'batch_p2p_comm': True, 'num_query_groups': 8, 'tokenizer': {'library': 'sentencepiece', 'type': None, 'model': './mistral-7B-hf/tokenizer.model', 'vocab_file': None, 'merge_file': None, 'delimiter': None, 'sentencepiece_legacy': False}, 'native_amp_init_scale': 4294967296, 'native_amp_growth_interval': 1000, 'hysteresis': 2, 'fp32_residual_connection': False, 'fp16_lm_cross_entropy': False, 'megatron_amp_O2': False, 'grad_allreduce_chunk_size_mb': 125, 'grad_div_ar_fusion': True, 'gradient_accumulation_fusion': False, 'bias_activation_fusion': False, 'bias_dropout_add_fusion': False, 'masked_softmax_fusion': True, 'get_attention_mask_from_fusion': True, 'apply_rope_fusion': False, 'seed': 1234, 'resume_from_checkpoint': None, 'use_cpu_initialization': True, 'onnx_safe': False, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'sync_batch_comm': False, 'activations_checkpoint_granularity': None, 'activations_checkpoint_method': None, 'activations_checkpoint_num_layers': None, 'num_micro_batches_with_partial_activation_checkpoints': None, 'activations_checkpoint_layers_per_pipeline': None, 'sequence_parallel': False, 'transformer_engine': True, 'fp8': False, 'fp8_e4m3': False, 'fp8_hybrid': True, 'fp8_margin': 0, 'fp8_interval': 1, 'fp8_amax_history_len': 1024, 'fp8_amax_compute_algo': 'max', 'reduce_amax': True, 'use_emha': False, 'data': {'index_mapping_dir': None, 'data_impl': 'mmap', 'splits_string': '900,50,50', 'seq_length': '${model.encoder_seq_length}', 'skip_warmup': True, 'num_workers': 2, 'dataloader_type': 'single', 'reset_position_ids': False, 'reset_attention_mask': False, 'eod_mask_loss': False, 'validation_drop_last': True, 'no_seqlen_plus_one_input_tokens': False, 'pad_samples_to_global_batch_size': False, 'shuffle_documents': True}, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'optim': {'name': 'fused_adam', 'lr': 0.0002, 'weight_decay': 0.01, 'betas': [0.9, 0.98], 'sched': {'name': 'CosineAnnealing', 'warmup_steps': 500, 'constant_steps': 50000, 'min_lr': 2e-05}}, 'window_size': [4096, 0], 'rotary_base': 10000.0, 'precision': 'bf16'}\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-10-16 20:08:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "    \n",
      "converting layer 0\n",
      "done layer 0\n",
      "converting layer 1\n",
      "done layer 1\n",
      "converting layer 2\n",
      "done layer 2\n",
      "converting layer 3\n",
      "done layer 3\n",
      "converting layer 4\n",
      "done layer 4\n",
      "converting layer 5\n",
      "done layer 5\n",
      "converting layer 6\n",
      "done layer 6\n",
      "converting layer 7\n",
      "done layer 7\n",
      "converting layer 8\n",
      "done layer 8\n",
      "converting layer 9\n",
      "done layer 9\n",
      "converting layer 10\n",
      "done layer 10\n",
      "converting layer 11\n",
      "done layer 11\n",
      "converting layer 12\n",
      "done layer 12\n",
      "converting layer 13\n",
      "done layer 13\n",
      "converting layer 14\n",
      "done layer 14\n",
      "converting layer 15\n",
      "done layer 15\n",
      "converting layer 16\n",
      "done layer 16\n",
      "converting layer 17\n",
      "done layer 17\n",
      "converting layer 18\n",
      "done layer 18\n",
      "converting layer 19\n",
      "done layer 19\n",
      "converting layer 20\n",
      "done layer 20\n",
      "converting layer 21\n",
      "done layer 21\n",
      "converting layer 22\n",
      "done layer 22\n",
      "converting layer 23\n",
      "done layer 23\n",
      "converting layer 24\n",
      "done layer 24\n",
      "converting layer 25\n",
      "done layer 25\n",
      "converting layer 26\n",
      "done layer 26\n",
      "converting layer 27\n",
      "done layer 27\n",
      "converting layer 28\n",
      "done layer 28\n",
      "converting layer 29\n",
      "done layer 29\n",
      "converting layer 30\n",
      "done layer 30\n",
      "converting layer 31\n",
      "done layer 31\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:294] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:303] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:316] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:355] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:357] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-10-16 20:08:07 tokenizer_utils:193] Getting SentencePiece with model: /workspace/mistral-7B-hf/tokenizer.model\n",
      "[NeMo I 2024-10-16 20:08:07 megatron_base_model:595] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:1182] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:510] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:08:07 megatron_base_model:568] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=gloo\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2024-10-16 20:08:50 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo I 2024-10-16 20:09:34 convert_mistral_7b_hf_to_nemo:349] NeMo model saved to: mistral.nemo\n",
      "model conversion finished\n",
      "downloading datasets into folder\n",
      "Cloning into 'databricks-dolly-15k'...\n",
      "remote: Enumerating objects: 36, done.\u001b[K\n",
      "remote: Total 36 (delta 0), reused 0 (delta 0), pack-reused 36 (from 1)\u001b[K\n",
      "Unpacking objects: 100% (36/36), 8.30 KiB | 1.38 MiB/s, done.\n",
      "--2024-10-16 20:09:39--  https://huggingface.co/datasets/databricks/databricks-dolly-15k/resolve/main/databricks-dolly-15k.jsonl\n",
      "Resolving huggingface.co (huggingface.co)... 108.156.211.95, 108.156.211.51, 108.156.211.90, ...\n",
      "Connecting to huggingface.co (huggingface.co)|108.156.211.95|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.hf.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1729368579&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyOTM2ODU3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=b9reWhHNgciaEefy-gE7fhZuvoVSsnWnGQZEg-30TQG23bzIJqGshv5LUQVAM3ZIPEC7QgFx7XHK1InCHFJcEJ8PVKuZ7I%7EySfy%7E3hobb2UhtOmPaalzlFhKsqeBpuQeZ3HZnQ-5H0kraUdS4LEfElf%7E9NkjywZka1T1xwGpFkg-BmLVJr64t3PtACv4zrduEd5ldLLb4SRU6WiOdXSoNKihQEXBEBNELxr5jzl6dVTHoTXRogY5y5yRxs8v8qD38VU7U1BVtsk2U5HdxcdGoJ9yuitxZc2JttJM%7EiJ3Ox7iRJf7X8JAoq7TZKgxgoaja02TSe8QIJAd1z8Gs4jxzA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
      "--2024-10-16 20:09:39--  https://cdn-lfs.hf.co/repos/34/ac/34ac588cc580830664f592597bb6d19d61639eca33dc2d6bb0b6d833f7bfd552/2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27databricks-dolly-15k.jsonl%3B+filename%3D%22databricks-dolly-15k.jsonl%22%3B&Expires=1729368579&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyOTM2ODU3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8zNC9hYy8zNGFjNTg4Y2M1ODA4MzA2NjRmNTkyNTk3YmI2ZDE5ZDYxNjM5ZWNhMzNkYzJkNmJiMGI2ZDgzM2Y3YmZkNTUyLzJkZjkwODMzMzhiNGFiZDZiY2ViNTYzNTc2NGRhYjVkODMzYjM5M2I1NTc1OWRmZmIwOTU5YjZmY2JmNzk0ZWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=b9reWhHNgciaEefy-gE7fhZuvoVSsnWnGQZEg-30TQG23bzIJqGshv5LUQVAM3ZIPEC7QgFx7XHK1InCHFJcEJ8PVKuZ7I%7EySfy%7E3hobb2UhtOmPaalzlFhKsqeBpuQeZ3HZnQ-5H0kraUdS4LEfElf%7E9NkjywZka1T1xwGpFkg-BmLVJr64t3PtACv4zrduEd5ldLLb4SRU6WiOdXSoNKihQEXBEBNELxr5jzl6dVTHoTXRogY5y5yRxs8v8qD38VU7U1BVtsk2U5HdxcdGoJ9yuitxZc2JttJM%7EiJ3Ox7iRJf7X8JAoq7TZKgxgoaja02TSe8QIJAd1z8Gs4jxzA__&Key-Pair-Id=K3RPWS32NSSJCE\n",
      "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 13.249.59.33, 13.249.59.48, 13.249.59.109, ...\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|13.249.59.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13085339 (12M) [text/plain]\n",
      "Saving to: ‘databricks-dolly-15k/databricks-dolly-15k.jsonl’\n",
      "\n",
      "databricks-dolly-15 100%[===================>]  12.48M  69.1MB/s    in 0.2s    \n",
      "\n",
      "2024-10-16 20:09:39 (69.1 MB/s) - ‘databricks-dolly-15k/databricks-dolly-15k.jsonl’ saved [13085339/13085339]\n",
      "\n",
      "13M\tdatabricks-dolly-15k/databricks-dolly-15k.jsonl\n",
      "2df9083338b4abd6bceb5635764dab5d833b393b55759dffb0959b6fcbf794ec  databricks-dolly-15k/databricks-dolly-15k.jsonl\n",
      "preprocess data sources to follow correct format\n",
      "Preprocessing data to jsonl format...\n",
      "Data was successfully preprocessed and saved by databricks-dolly-15k/databricks-dolly-15k-output.jsonl .\n",
      "checking if jsonl files exist!\n",
      "README.md  databricks-dolly-15k-output.jsonl  databricks-dolly-15k.jsonl\n",
      "check first three examples in the output jsonl file!\n",
      "{\"input\": \"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia's domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\\n\\nWhen did Virgin Australia start operating?\", \"output\": \"Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.\", \"category\": \"closed_qa\"}\n",
      "{\"input\": \"Which is a species of fish? Tope or Rope\", \"output\": \"Tope\", \"category\": \"classification\"}\n",
      "{\"input\": \"Why can camels survive for long without water?\", \"output\": \"Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.\", \"category\": \"open_qa\"}\n",
      "generating train validation test dataset\n",
      "finished writing file:  databricks-dolly-15k/train.jsonl\n",
      "finished writing file:  databricks-dolly-15k/validation.jsonl\n",
      "finished writing file:  databricks-dolly-15k/test.jsonl\n",
      "finish generating train, validation and test data\n",
      "check train val test data are generated\n",
      "check train, val, test data are generated\n",
      "README.md\t\t\t   databricks-dolly-15k.jsonl  train.jsonl\n",
      "databricks-dolly-15k-output.jsonl  test.jsonl\t\t       validation.jsonl\n",
      "running supervised fine tuning step...\n",
      "[2024-10-16 20:09:41,375] torch.distributed.run: [WARNING] \n",
      "[2024-10-16 20:09:41,375] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-10-16 20:09:41,375] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-10-16 20:09:41,375] torch.distributed.run: [WARNING] *****************************************\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-10-16 20:09:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2024-10-16 20:09:53 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-16 20:09:53 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 8\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 5\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 0.1\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /results\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_loss\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: false\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 8\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 128\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: mistral.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: true\n",
      "      activations_checkpoint_granularity: selective\n",
      "      activations_checkpoint_method: uniform\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: none\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - databricks-dolly-15k/train.jsonl\n",
      "          global_batch_size: 128\n",
      "          micro_batch_size: 1\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - databricks-dolly-15k/validation.jsonl\n",
      "          names: null\n",
      "          global_batch_size: 128\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - databricks-dolly-15k/test.jsonl\n",
      "          names: null\n",
      "          global_batch_size: 256\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: distributed_fused_adam\n",
      "        lr: 1.0e-06\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    cluster_type: BCP\n",
      "    \n",
      "[NeMo W 2024-10-16 20:09:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2024-10-16 20:09:53 exp_manager:396] ExpManager schema\n",
      "[NeMo I 2024-10-16 20:09:53 exp_manager:397] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}}\n",
      "[NeMo W 2024-10-16 20:09:53 exp_manager:835] Exp_manager is logging to /results, but it already exists.\n",
      "[NeMo W 2024-10-16 20:09:53 exp_manager:757] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/results/checkpoints. Training from scratch.\n",
      "[NeMo I 2024-10-16 20:09:53 exp_manager:455] Experiments will be logged at /results\n",
      "[NeMo I 2024-10-16 20:09:53 exp_manager:983] TensorboardLogger has been set up\n",
      "[NeMo W 2024-10-16 20:09:53 exp_manager:1111] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 5. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:280] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:294] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:302] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:303] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:312] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:316] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:355] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:357] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-10-16 20:10:19 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-10-16 20:10:19 tokenizer_utils:193] Getting SentencePiece with model: /tmp/tmpfzll2zva/adcc39c734d145f1ad0b7253767e5180_tokenizer.model\n",
      "[NeMo I 2024-10-16 20:10:19 megatron_base_model:595] Padded vocab_size: 32768, original vocab_size: 32000, dummy tokens: 768.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:510] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:19 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:10:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:582: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
      "      warnings.warn(\n",
      "    \n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 8 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank0]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "[rank6]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "[rank5]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[rank2]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "[rank4]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "[rank3]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "[rank1]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "[rank7]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[NeMo W 2024-10-16 20:11:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:27: UserWarning: 'load_state_dict' is deprecated and will be removed in future versions. Please use 'load' instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      warnings.warn(DEPRECATE_MSG)\n",
      "    \n",
      "[NeMo I 2024-10-16 20:11:13 nlp_overrides:1346] Model MegatronGPTSFTModel was successfully restored from /workspace/mistral.nemo.\n",
      "[NeMo I 2024-10-16 20:11:13 megatron_gpt_finetuning:75] Running full finetuning since no peft scheme is given.\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 906 M  | train\n",
      "    ------------------------------------------------\n",
      "    906 M     Trainable params\n",
      "    0         Non-trainable params\n",
      "    906 M     Total params\n",
      "    3,624.944 Total estimated model params size (MB)\n",
      "[NeMo W 2024-10-16 20:11:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "[NeMo I 2024-10-16 20:11:13 megatron_gpt_sft_model:801] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:495] Building indexing for fn = databricks-dolly-15k/validation.jsonl\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:507] Saving idx file = databricks-dolly-15k/validation.jsonl.idx.npy\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:509] Saving metadata file = databricks-dolly-15k/validation.jsonl.idx.info\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.056140\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.088081\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:249] Loading databricks-dolly-15k/validation.jsonl\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001020\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-10-16 20:11:13 megatron_gpt_sft_model:805] Length of val dataset: 2252\n",
      "[NeMo I 2024-10-16 20:11:13 megatron_gpt_sft_model:793] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:495] Building indexing for fn = databricks-dolly-15k/test.jsonl\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:507] Saving idx file = databricks-dolly-15k/test.jsonl.idx.npy\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:509] Saving metadata file = databricks-dolly-15k/test.jsonl.idx.info\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.054228\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.051605\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:249] Loading databricks-dolly-15k/test.jsonl\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000808\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-10-16 20:11:13 megatron_gpt_sft_model:796] Length of test dataset: 751\n",
      "[NeMo I 2024-10-16 20:11:13 megatron_gpt_sft_model:812] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:495] Building indexing for fn = databricks-dolly-15k/train.jsonl\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:507] Saving idx file = databricks-dolly-15k/train.jsonl.idx.npy\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:509] Saving metadata file = databricks-dolly-15k/train.jsonl.idx.info\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:535] Time building 1 / 1 mem-mapped files: 0:00:00.063156\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:535] Time building 0 / 1 mem-mapped files: 0:00:00.052165\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:249] Loading databricks-dolly-15k/train.jsonl\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000841\n",
      "[NeMo I 2024-10-16 20:11:13 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo W 2024-10-16 20:11:13 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:79.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n",
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-10-16 20:11:17 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.05 (sec)\n",
      "[NeMo I 2024-10-16 20:11:17 megatron_gpt_sft_model:814] Length of train dataset: 644\n",
      "[NeMo I 2024-10-16 20:11:17 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-16 20:11:17 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-16 20:11:17 megatron_gpt_sft_model:819] Building dataloader with consumed samples: 0\n",
      "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "[NeMo W 2024-10-16 20:11:17 megatron_base_model:1223] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 5.\n",
      "[NeMo I 2024-10-16 20:11:17 modelPT:786] Optimizer config = MegatronDistributedFusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        is_expert: False\n",
      "        lr: 1e-06\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-10-16 20:11:17 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fdea9d8dd50>\" \n",
      "    will be used during training (effective maximum steps = 5) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 5\n",
      "    )\n",
      "[NeMo W 2024-10-16 20:11:17 nemo_logging:349] /opt/apex/apex/contrib/optimizers/distributed_fused_adam.py:1488: UserWarning: Only 53.2% of buckets are used. Consider decreasing the bucket_cap_mb argument.\n",
      "      warnings.warn(\n",
      "    \n",
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | Float16Module | 906 M  | train\n",
      "------------------------------------------------\n",
      "906 M     Trainable params\n",
      "0         Non-trainable params\n",
      "906 M     Total params\n",
      "3,624.944 Total estimated model params size (MB)\n",
      "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s][NeMo W 2024-10-16 20:11:17 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:17 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo I 2024-10-16 20:11:17 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s][NeMo W 2024-10-16 20:11:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:3296: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:23 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:638: UserWarning: When using sequence parallelism it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:23 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py:2887: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:25<00:00,  0.08it/s][NeMo I 2024-10-16 20:11:42 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "[NeMo W 2024-10-16 20:11:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:439: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:149: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-10-16 20:11:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "    \n",
      "Epoch 0: :  20%|▏| 1/5 [00:23<01:35, reduced_train_loss=1.760, global_step=0.000\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-16 20:12:06 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Validation:   0%|                                        | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 1/18 [00:10<02:51,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 2/18 [00:20<02:41,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 3/18 [00:29<02:27,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▏              | 4/18 [00:38<02:13,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████▎             | 5/18 [00:47<02:02,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 6/18 [00:57<01:54,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████▍           | 7/18 [01:07<01:45,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▍          | 8/18 [01:16<01:36,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 9/18 [01:25<01:25,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 10/18 [01:35<01:16,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 11/18 [01:43<01:06,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 12/18 [01:52<00:56,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 13/18 [02:01<00:46,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 14/18 [02:11<00:37,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 15/18 [02:21<00:28,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 16/18 [02:31<00:18,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 17/18 [02:41<00:09,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 18/18 [02:50<00:00,  0.11it/s]\u001b[A[NeMo I 2024-10-16 20:14:57 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Epoch 0: :  20%|▏| 1/5 [03:14<12:58, reduced_train_loss=1.760, global_step=0.000[rank: 0] Metric val_loss improved. New best score: 1.726\n",
      "[rank: 7] Metric val_loss improved. New best score: 1.726\n",
      "[rank: 5] Metric val_loss improved. New best score: 1.726\n",
      "[rank: 4] Metric val_loss improved. New best score: 1.726\n",
      "[rank: 3] Metric val_loss improved. New best score: 1.726\n",
      "[rank: 1] Metric val_loss improved. New best score: 1.726\n",
      "[rank: 6] Metric val_loss improved. New best score: 1.726\n",
      "[rank: 2] Metric val_loss improved. New best score: 1.726\n",
      "Epoch 0, global step 1: 'validation_loss' reached 1.72608 (best 1.72608), saving model to '/results/checkpoints/megatron_gpt_peft_none_tuning--validation_loss=1.726-step=1-consumed_samples=128.0.ckpt' as top 1\n",
      "[NeMo W 2024-10-16 20:15:00 nlp_overrides:604] Distributed checkpoints requires DistributedCheckpointIO plugin to be used. Setting up a default now.\n",
      "[NeMo I 2024-10-16 20:15:00 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "Epoch 0: :  40%|▍| 2/5 [05:09<07:44, reduced_train_loss=1.750, global_step=1.000\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-16 20:16:52 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Validation:   0%|                                        | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 1/18 [00:09<02:47,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 2/18 [00:19<02:37,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 3/18 [00:28<02:24,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▏              | 4/18 [00:37<02:11,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████▎             | 5/18 [00:46<02:01,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 6/18 [00:56<01:52,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████▍           | 7/18 [01:06<01:44,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▍          | 8/18 [01:16<01:35,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 9/18 [01:25<01:25,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 10/18 [01:35<01:16,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 11/18 [01:43<01:05,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 12/18 [01:51<00:55,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 13/18 [02:01<00:46,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 14/18 [02:11<00:37,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 15/18 [02:20<00:28,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 16/18 [02:30<00:18,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 17/18 [02:40<00:09,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 18/18 [02:50<00:00,  0.11it/s]\u001b[A[NeMo I 2024-10-16 20:19:42 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Epoch 0: :  40%|▍| 2/5 [08:00<12:00, reduced_train_loss=1.750, global_step=1.000Epoch 0, global step 2: 'validation_loss' reached 1.72594 (best 1.72594), saving model to '/results/checkpoints/megatron_gpt_peft_none_tuning--validation_loss=1.726-step=2-consumed_samples=256.0.ckpt' as top 1\n",
      "Epoch 0: :  60%|▌| 3/5 [10:32<07:01, reduced_train_loss=1.810, global_step=2.000\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-16 20:22:15 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Validation:   0%|                                        | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 1/18 [00:09<02:48,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 2/18 [00:19<02:38,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 3/18 [00:29<02:25,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▏              | 4/18 [00:37<02:11,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████▎             | 5/18 [00:46<02:01,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 6/18 [00:56<01:53,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████▍           | 7/18 [01:06<01:44,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▍          | 8/18 [01:16<01:35,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 9/18 [01:25<01:25,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 10/18 [01:35<01:16,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 11/18 [01:43<01:05,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 12/18 [01:51<00:55,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 13/18 [02:01<00:46,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 14/18 [02:11<00:37,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 15/18 [02:21<00:28,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 16/18 [02:30<00:18,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 17/18 [02:40<00:09,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 18/18 [02:50<00:00,  0.11it/s]\u001b[A[NeMo I 2024-10-16 20:25:05 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Epoch 0: :  60%|▌| 3/5 [13:23<08:55, reduced_train_loss=1.810, global_step=2.000Epoch 0, global step 3: 'validation_loss' reached 1.72574 (best 1.72574), saving model to '/results/checkpoints/megatron_gpt_peft_none_tuning--validation_loss=1.726-step=3-consumed_samples=384.0.ckpt' as top 1\n",
      "Epoch 0: :  80%|▊| 4/5 [16:08<04:02, reduced_train_loss=1.650, global_step=3.000\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-16 20:27:51 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Validation:   0%|                                        | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 1/18 [00:09<02:49,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 2/18 [00:19<02:39,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 3/18 [00:29<02:26,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▏              | 4/18 [00:37<02:12,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████▎             | 5/18 [00:47<02:02,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 6/18 [00:57<01:54,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████▍           | 7/18 [01:06<01:45,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▍          | 8/18 [01:16<01:36,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 9/18 [01:26<01:26,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 10/18 [01:35<01:16,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 11/18 [01:44<01:06,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 12/18 [01:52<00:56,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 13/18 [02:02<00:47,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 14/18 [02:12<00:37,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 15/18 [02:22<00:28,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 16/18 [02:32<00:19,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 17/18 [02:41<00:09,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 18/18 [02:51<00:00,  0.10it/s]\u001b[A[NeMo I 2024-10-16 20:30:43 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Epoch 0: :  80%|▊| 4/5 [19:00<04:45, reduced_train_loss=1.650, global_step=3.000[rank: 0] Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.725\n",
      "[rank: 5] Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.725\n",
      "[rank: 6] Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.725\n",
      "[rank: 3] Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.725\n",
      "[rank: 2] Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.725\n",
      "[rank: 7] Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.725\n",
      "[rank: 4] Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.725\n",
      "[rank: 1] Metric val_loss improved by 0.001 >= min_delta = 0.001. New best score: 1.725\n",
      "Epoch 0, global step 4: 'validation_loss' reached 1.72499 (best 1.72499), saving model to '/results/checkpoints/megatron_gpt_peft_none_tuning--validation_loss=1.725-step=4-consumed_samples=512.0.ckpt' as top 1\n",
      "[NeMo W 2024-10-16 20:32:28 gpt_sft_dataset:324] input is not long enough to truncate.\n",
      "[NeMo W 2024-10-16 20:32:28 gpt_sft_dataset:413] After truncation, input ids length 3403 still exceeds max sequence length 2048\n",
      "Epoch 0: : 100%|█| 5/5 [21:10<00:00, reduced_train_loss=1.800, global_step=4.000\n",
      "Validation: |                                             | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-16 20:32:52 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Validation:   0%|                                        | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                           | 0/18 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|█                  | 1/18 [00:09<02:47,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|██                 | 2/18 [00:19<02:39,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|███▏               | 3/18 [00:29<02:26,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|████▏              | 4/18 [00:37<02:12,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|█████▎             | 5/18 [00:46<02:02,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|██████▎            | 6/18 [00:56<01:53,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███████▍           | 7/18 [01:06<01:44,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|████████▍          | 8/18 [01:16<01:35,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████████▌         | 9/18 [01:25<01:25,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|██████████        | 10/18 [01:35<01:16,  0.10it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|███████████       | 11/18 [01:43<01:06,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|████████████      | 12/18 [01:52<00:56,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|█████████████     | 13/18 [02:01<00:46,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|██████████████    | 14/18 [02:11<00:37,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%|███████████████   | 15/18 [02:21<00:28,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%|████████████████  | 16/18 [02:31<00:18,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████████████ | 17/18 [02:41<00:09,  0.11it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████| 18/18 [02:50<00:00,  0.11it/s]\u001b[A[NeMo I 2024-10-16 20:35:43 num_microbatches_calculator:119] setting number of micro-batches to constant 128\n",
      "\n",
      "Epoch 0: : 100%|█| 5/5 [24:00<00:00, reduced_train_loss=1.800, global_step=4.000[rank: 0] Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.723\n",
      "[rank: 5] Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.723\n",
      "[rank: 2] Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.723\n",
      "[rank: 4] Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.723\n",
      "[rank: 1] Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.723\n",
      "[rank: 3] Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.723\n",
      "[rank: 7] Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.723\n",
      "[rank: 6] Metric val_loss improved by 0.002 >= min_delta = 0.001. New best score: 1.723\n",
      "Epoch 0, global step 5: 'validation_loss' reached 1.72336 (best 1.72336), saving model to '/results/checkpoints/megatron_gpt_peft_none_tuning--validation_loss=1.723-step=5-consumed_samples=640.0.ckpt' as top 1\n",
      "Epoch 0: : 100%|█| 5/5 [26:10<00:00, reduced_train_loss=1.800, global_step=4.000`Trainer.fit` stopped: `max_steps=5` reached.\n",
      "Epoch 0: : 100%|█| 5/5 [26:10<00:00, reduced_train_loss=1.800, global_step=4.000\n",
      "[NeMo I 2024-10-16 20:37:53 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n",
      "code/run_sft.sh: line 94: unexpected EOF while looking for matching `\"'\n",
      "code/run_sft.sh: line 97: syntax error: unexpected end of file\n"
     ]
    }
   ],
   "source": [
    "!bash code/run_sft.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a350a-5836-4d93-bfd9-15e7c74081b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# run script to test the supervised fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a592c178-7bde-473a-9e59-81df99e7fb0d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing model testing after sft\n",
      "`zarr` distributed checkpoint backend is deprecated. Please switch to PyTorch Distributed format (`torch_dist`).\n",
      "[NeMo W 2024-10-16 20:43:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2024-10-16 20:43:44 megatron_gpt_generate:125] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-16 20:43:44 megatron_gpt_generate:126] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 1\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: /results/checkpoints/megatron_gpt_peft_none_tuning.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: null\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names: test.jsonl\n",
      "          names:\n",
      "          - dolly-15k_test\n",
      "          global_batch_size: 16\n",
      "          micro_batch_size: 2\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: ${data.train_ds.label_key}\n",
      "          add_eos: ${data.train_ds.add_eos}\n",
      "          add_sep: ${data.train_ds.add_sep}\n",
      "          add_bos: ${data.train_ds.add_bos}\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: /results/sft_results\n",
      "          truncation_field: ${data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 20\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n",
      "[NeMo W 2024-10-16 20:43:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:269] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:275] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:280] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:283] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:291] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:294] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:295] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:302] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:303] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:312] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:316] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:317] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:337] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:349] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:355] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:356] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:357] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_init:358] Rank 0 has embedding rank: 0\n",
      "setting number of micro-batches to constant 1\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2024-10-16 20:43:57 tokenizer_utils:193] Getting SentencePiece with model: /tmp/tmpod73cqay/adcc39c734d145f1ad0b7253767e5180_tokenizer.model\n",
      "[NeMo I 2024-10-16 20:43:57 megatron_base_model:595] Padded vocab_size: 32000, original vocab_size: 32000, dummy tokens: 0.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:1182] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-16 20:43:57 megatron_base_model:568] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank0]:[W ProcessGroupGloo.cpp:721] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[NeMo W 2024-10-16 20:44:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:27: UserWarning: 'load_state_dict' is deprecated and will be removed in future versions. Please use 'load' instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-10-16 20:44:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/_shard/sharded_tensor/api.py:1132: UserWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      warnings.warn(DEPRECATE_MSG)\n",
      "    \n",
      "[NeMo I 2024-10-16 20:44:27 nlp_overrides:1346] Model MegatronGPTSFTModel was successfully restored from /results/checkpoints/megatron_gpt_peft_none_tuning.nemo.\n",
      "[NeMo I 2024-10-16 20:44:27 megatron_gpt_generate:156] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type     | Params | Mode\n",
      "    ------------------------------------------\n",
      "    0 | model | GPTModel | 7.2 B  | eval\n",
      "    ------------------------------------------\n",
      "    0         Trainable params\n",
      "    7.2 B     Non-trainable params\n",
      "    7.2 B     Total params\n",
      "    28,966.928Total estimated model params size (MB)\n",
      "[NeMo W 2024-10-16 20:44:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-16 20:44:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "[NeMo I 2024-10-16 20:44:27 megatron_gpt_sft_model:793] Building GPT SFT test datasets.\n",
      "Error executing job with overrides: ['model.restore_from_path=/results/checkpoints/megatron_gpt_peft_none_tuning.nemo', 'trainer.devices=1', 'model.data.test_ds.file_names=test.jsonl', 'model.data.test_ds.names=[dolly-15k_test]', 'model.data.test_ds.global_batch_size=16', 'model.data.test_ds.micro_batch_size=2', 'model.data.test_ds.tokens_to_generate=20', 'model.tensor_model_parallel_size=1', 'model.pipeline_model_parallel_size=1', 'inference.greedy=True', 'model.data.test_ds.output_file_path_prefix=/results/sft_results', 'model.data.test_ds.write_predictions_to_file=True']\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py\", line 164, in main\n",
      "    trainer.test(model)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 753, in test\n",
      "    return call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 43, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 793, in _test_impl\n",
      "    results = self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 948, in _run\n",
      "    call._call_setup_hook(self)  # allow user to set up LightningModule in accelerator environment\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 96, in _call_setup_hook\n",
      "    _call_lightning_module_hook(trainer, \"setup\", stage=fn)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 159, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_sft_model.py\", line 175, in setup\n",
      "    self.build_train_valid_test_datasets(stage=stage)\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_sft_model.py\", line 808, in build_train_valid_test_datasets\n",
      "    self.maybe_build_test()\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_sft_model.py\", line 795, in maybe_build_test\n",
      "    self._test_ds = self._build_dataset(self.cfg.data.test_ds, is_train=False)\n",
      "  File \"/opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_gpt_sft_model.py\", line 196, in _build_dataset\n",
      "    raise ValueError(f\"SFT train/validation datasets must be provided as a list of individual JSONL files.\")\n",
      "ValueError: SFT train/validation datasets must be provided as a list of individual JSONL files.\n",
      "\n",
      "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
      "finished testing model, here are some sample results: \n",
      "tail: cannot open '/results/sft_results.jsonl' for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!bash code/test_sft_model.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
