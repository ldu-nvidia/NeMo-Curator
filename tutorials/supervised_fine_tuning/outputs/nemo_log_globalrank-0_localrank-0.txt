[NeMo W 2024-11-27 00:49:33 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(ctx, input, weight, bias, allreduce_dgrad):
    
[NeMo W 2024-11-27 00:49:33 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, grad_output):
    
[NeMo W 2024-11-27 00:49:33 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
      def forward(
    
[NeMo W 2024-11-27 00:49:33 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
      def backward(ctx, grad_output):
    
[NeMo W 2024-11-27 00:49:34 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead
      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor
    
[NeMo W 2024-11-27 00:49:34 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' 
        module is deprecated and will be removed in 0.10.0. Please use 
        'megatron.core.extensions.transformer_engine' instead.
      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim
    
[NeMo W 2024-11-27 00:49:35 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_base_prompt_learning_model.py:181: DeprecationWarning: invalid escape sequence '\{'
      "prompt_template_fields": re.findall("\{(.*?)\}", task.prompt_template),
    
[NeMo W 2024-11-27 00:49:35 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_base_model.py:389: DeprecationWarning: invalid escape sequence '\.'
      return re.fullmatch("[0-9][0-9]\.[0-9][0-9].*", nvidia_torch_version)  # "YY.MM.*"
    
[NeMo W 2024-11-27 00:49:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
      quantize_op_abstract = torch.library.impl_abstract("tensorrt::quantize_op")(
    
[NeMo W 2024-11-27 00:49:36 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/megatron/vocab_parallel_cross_entropy.py:88: DeprecationWarning: invalid escape sequence '\s'
      """
    
[NeMo W 2024-11-27 00:49:37 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/wfst_utils.py:1328: DeprecationWarning: invalid escape sequence '\d'
      width, height = re.findall('\d+', line)
    
[NeMo W 2024-11-27 00:49:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.
      cm = get_cmap("Set1")
    
[NeMo W 2024-11-27 00:49:37 nemo_logging:349] /opt/NeMo/nemo/collections/asr/modules/rnnt.py:1558: DeprecationWarning: invalid escape sequence '\*'
      """
    
[NeMo W 2024-11-27 00:49:37 nemo_logging:349] /opt/NeMo/nemo/collections/common/data/lhotse/nemo_adapters.py:198: DeprecationWarning: invalid escape sequence '\d'
      """
    
[NeMo W 2024-11-27 00:49:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
      if get_gast_version() < LooseVersion("0.5"):
    
[NeMo W 2024-11-27 00:49:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
      other = LooseVersion(other)
    
[NeMo W 2024-11-27 00:49:38 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/vad_utils.py:1109: DeprecationWarning: invalid escape sequence '\s'
      data = pd.read_csv(path2ground_truth_label, sep="\s+", delimiter=None, header=None)
    
[NeMo W 2024-11-27 00:49:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
      ret = run_job(
    
[NeMo I 2024-11-27 00:49:38 megatron_gpt_finetuning:56] 
    
    ************** Experiment configuration ***********
[NeMo I 2024-11-27 00:49:38 megatron_gpt_finetuning:57] 
    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning
    trainer:
      devices: 2
      accelerator: gpu
      num_nodes: 1
      precision: bf16
      logger: false
      enable_checkpointing: false
      use_distributed_sampler: false
      max_epochs: 9999
      max_steps: 5
      log_every_n_steps: 10
      val_check_interval: 0.1
      gradient_clip_val: 1.0
    exp_manager:
      explicit_log_dir: /results
      exp_dir: null
      name: ${name}
      create_wandb_logger: false
      wandb_logger_kwargs:
        project: null
        name: null
      resume_if_exists: true
      resume_ignore_no_checkpoint: true
      create_checkpoint_callback: true
      checkpoint_callback_params:
        monitor: validation_loss
        save_top_k: 1
        mode: min
        save_nemo_on_train_end: true
        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}
        model_parallel_size: ${model.tensor_model_parallel_size}
        always_save_nemo: false
        save_best_model: false
      create_early_stopping_callback: true
      early_stopping_callback_params:
        monitor: val_loss
        mode: min
        min_delta: 0.001
        patience: 10
        verbose: true
        strict: false
    model:
      seed: 1234
      tensor_model_parallel_size: 2
      pipeline_model_parallel_size: 1
      global_batch_size: 128
      micro_batch_size: 1
      restore_from_path: ../mistral-7b.nemo
      resume_from_checkpoint: null
      save_nemo_on_validation_end: false
      sync_batch_comm: false
      megatron_amp_O2: true
      sequence_parallel: true
      activations_checkpoint_granularity: selective
      activations_checkpoint_method: uniform
      activations_checkpoint_num_layers: null
      activations_checkpoint_layers_per_pipeline: null
      answer_only_loss: true
      gradient_as_bucket_view: false
      hidden_dropout: 0.0
      attention_dropout: 0.0
      ffn_dropout: 0.0
      fsdp: false
      fsdp_sharding_strategy: full
      fsdp_grad_reduce_dtype: fp32
      fsdp_sharded_checkpoint: false
      fsdp_use_orig_params: false
      peft:
        peft_scheme: none
        restore_from_path: null
        adapter_tuning:
          type: parallel_adapter
          adapter_dim: 32
          adapter_dropout: 0.0
          norm_position: pre
          column_init_method: xavier
          row_init_method: zero
          norm_type: mixedfusedlayernorm
          layer_selection: null
          weight_tying: false
          position_embedding_strategy: null
        lora_tuning:
          variant: nemo
          target_modules:
          - attention_qkv
          adapter_dim: 32
          alpha: ${model.peft.lora_tuning.adapter_dim}
          adapter_dropout: 0.0
          column_init_method: xavier
          row_init_method: zero
          layer_selection: null
          weight_tying: false
          position_embedding_strategy: null
        p_tuning:
          virtual_tokens: 10
          bottleneck_dim: 1024
          embedding_dim: 1024
          init_std: 0.023
        ia3_tuning:
          layer_selection: null
        selective_tuning:
          tunable_base_param_names:
          - self_attention
          - word_embeddings
      data:
        train_ds:
          file_names:
          - data/merged/MG-Verilog_high_level_global_summary_in_out_train.jsonl
          global_batch_size: 128
          micro_batch_size: 1
          shuffle: true
          num_workers: 0
          memmap_workers: 2
          pin_memory: true
          max_seq_length: 2048
          min_seq_length: 1
          drop_last: true
          concat_sampling_probabilities:
          - 1.0
          label_key: output
          add_eos: true
          add_sep: false
          add_bos: false
          truncation_field: input
          index_mapping_dir: null
          prompt_template: '{input} {output}'
          truncation_method: right
          global_sample_mapping: false
        validation_ds:
          file_names:
          - data/merged/MG-Verilog_high_level_global_summary_in_out_validation.jsonl
          names: null
          global_batch_size: 128
          micro_batch_size: 1
          shuffle: false
          num_workers: 0
          memmap_workers: ${model.data.train_ds.memmap_workers}
          pin_memory: true
          max_seq_length: 2048
          min_seq_length: 1
          drop_last: false
          label_key: ${model.data.train_ds.label_key}
          add_eos: ${model.data.train_ds.add_eos}
          add_sep: ${model.data.train_ds.add_sep}
          add_bos: ${model.data.train_ds.add_bos}
          write_predictions_to_file: false
          output_file_path_prefix: null
          truncation_field: ${model.data.train_ds.truncation_field}
          index_mapping_dir: null
          prompt_template: ${model.data.train_ds.prompt_template}
          tokens_to_generate: 32
          truncation_method: right
          global_sample_mapping: false
          metric:
            name: loss
            average: null
            num_classes: null
        test_ds:
          file_names:
          - data/merged/MG-Verilog_high_level_global_summary_in_out_test.jsonl
          names: null
          global_batch_size: 256
          micro_batch_size: 1
          shuffle: false
          num_workers: 0
          memmap_workers: ${model.data.train_ds.memmap_workers}
          pin_memory: true
          max_seq_length: 2048
          min_seq_length: 1
          drop_last: false
          label_key: ${model.data.train_ds.label_key}
          add_eos: ${model.data.train_ds.add_eos}
          add_sep: ${model.data.train_ds.add_sep}
          add_bos: ${model.data.train_ds.add_bos}
          write_predictions_to_file: false
          output_file_path_prefix: null
          truncation_field: ${model.data.train_ds.truncation_field}
          index_mapping_dir: null
          prompt_template: ${model.data.train_ds.prompt_template}
          tokens_to_generate: 32
          truncation_method: right
          global_sample_mapping: false
          metric:
            name: loss
            average: null
            num_classes: null
      optim:
        name: distributed_fused_adam
        lr: 1.0e-06
        weight_decay: 0.01
        betas:
        - 0.9
        - 0.98
        sched:
          name: CosineAnnealing
          warmup_steps: 50
          min_lr: 0.0
          constant_steps: 0
          monitor: val_loss
          reduce_on_plateau: false
    cluster_type: BCP
    
[NeMo W 2024-11-27 00:49:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.
    
[NeMo W 2024-11-27 00:49:38 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)
      torch.set_autocast_gpu_dtype(dtype)
    
[NeMo I 2024-11-27 00:49:38 exp_manager:400] ExpManager schema
[NeMo I 2024-11-27 00:49:38 exp_manager:401] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}
[NeMo W 2024-11-27 00:49:38 exp_manager:784] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/results/checkpoints. Training from scratch.
[NeMo I 2024-11-27 00:49:38 exp_manager:459] Experiments will be logged at /results
[NeMo I 2024-11-27 00:49:38 exp_manager:1010] TensorboardLogger has been set up
[NeMo W 2024-11-27 00:49:38 exp_manager:1139] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 5. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.
[NeMo I 2024-11-27 00:49:38 exp_manager:593] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2024-11-27 00:50:21 megatron_init:314] Rank 0 has data parallel group : [0]
[NeMo I 2024-11-27 00:50:21 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]
[NeMo I 2024-11-27 00:50:21 megatron_init:325] All data parallel group ranks with context parallel combined: [[0], [1]]
[NeMo I 2024-11-27 00:50:21 megatron_init:328] Ranks 0 has data parallel rank: 0
[NeMo I 2024-11-27 00:50:21 megatron_init:336] Rank 0 has context parallel group: [0]
[NeMo I 2024-11-27 00:50:21 megatron_init:339] All context parallel group ranks: [[0], [1]]
[NeMo I 2024-11-27 00:50:21 megatron_init:340] Ranks 0 has context parallel rank: 0
[NeMo I 2024-11-27 00:50:21 megatron_init:347] Rank 0 has model parallel group: [0, 1]
[NeMo I 2024-11-27 00:50:21 megatron_init:348] All model parallel group ranks: [[0, 1]]
[NeMo I 2024-11-27 00:50:21 megatron_init:357] Rank 0 has tensor model parallel group: [0, 1]
[NeMo I 2024-11-27 00:50:21 megatron_init:361] All tensor model parallel group ranks: [[0, 1]]
[NeMo I 2024-11-27 00:50:21 megatron_init:362] Rank 0 has tensor model parallel rank: 0
[NeMo I 2024-11-27 00:50:21 megatron_init:382] Rank 0 has pipeline model parallel group: [0]
[NeMo I 2024-11-27 00:50:21 megatron_init:394] Rank 0 has embedding group: [0]
[NeMo I 2024-11-27 00:50:21 megatron_init:400] All pipeline model parallel group ranks: [[0], [1]]
[NeMo I 2024-11-27 00:50:21 megatron_init:401] Rank 0 has pipeline model parallel rank 0
[NeMo I 2024-11-27 00:50:21 megatron_init:402] All embedding group ranks: [[0], [1]]
[NeMo I 2024-11-27 00:50:21 megatron_init:403] Rank 0 has embedding rank: 0
[NeMo I 2024-11-27 00:50:21 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo I 2024-11-27 00:50:21 tokenizer_utils:196] Getting SentencePiece with model: /tmp/tmpub_rh1aa/9bd71907fa754aff9d7d1fb0f5c4daa7_tokenizer.model
[NeMo I 2024-11-27 00:50:21 megatron_base_model:601] Padded vocab_size: 32768, original vocab_size: 32768, dummy tokens: 0.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:516] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.
[NeMo W 2024-11-27 00:50:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:718: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1
      warnings.warn(
    
[NeMo W 2024-11-27 00:51:03 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.
      checkpoint.load_state_dict(
    
[NeMo W 2024-11-27 00:51:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.
      device = getattr(value, "device", None)
    
[NeMo I 2024-11-27 00:51:26 nlp_overrides:1358] Model MegatronGPTSFTModel was successfully restored from /workspace/mistral-7b.nemo.
[NeMo I 2024-11-27 00:51:26 megatron_gpt_finetuning:75] Running full finetuning since no peft scheme is given.
      | Name  | Type          | Params | Mode 
    ------------------------------------------------
    0 | model | Float16Module | 3.6 B  | train
    ------------------------------------------------
    3.6 B     Trainable params
    0         Non-trainable params
    3.6 B     Total params
    14,496.580Total estimated model params size (MB)
    650       Modules in train mode
    0         Modules in eval mode
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      DESCRIPTOR = _descriptor.FileDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _descriptor.FieldDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      DESCRIPTOR = _descriptor.FileDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _descriptor.EnumValueDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _DATATYPE = _descriptor.EnumDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      DESCRIPTOR = _descriptor.FileDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _descriptor.FieldDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      DESCRIPTOR = _descriptor.FileDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _descriptor.FieldDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _TENSORPROTO = _descriptor.Descriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      DESCRIPTOR = _descriptor.FileDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:35: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _descriptor.EnumValueDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:29: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _DATACLASS = _descriptor.EnumDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:74: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _descriptor.FieldDescriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:67: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.
      _SUMMARYDESCRIPTION = _descriptor.Descriptor(
    
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
      np.bool8: (False, True),
    
[NeMo I 2024-11-27 00:51:26 megatron_gpt_sft_model:836] Building GPT SFT validation datasets.
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:116] Building data files
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:527] Processing 1 data files using 2 workers
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:493] Building indexing for fn = data/merged/MG-Verilog_high_level_global_summary_in_out_validation.jsonl
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:505] Saving idx file = data/merged/MG-Verilog_high_level_global_summary_in_out_validation.jsonl.idx.npy
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:507] Saving metadata file = data/merged/MG-Verilog_high_level_global_summary_in_out_validation.jsonl.idx.info
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:508: ResourceWarning: unclosed file <_io.BufferedWriter name='data/merged/MG-Verilog_high_level_global_summary_in_out_validation.jsonl.idx.info'>
      pickle.dump(data, open(idx_fn + ".info", "wb"))
    
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:542] Time building 1 / 1 mem-mapped files: 0:00:00.100399
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:527] Processing 1 data files using 2 workers
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.087451
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:158] Loading data files
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:249] Loading data/merged/MG-Verilog_high_level_global_summary_in_out_validation.jsonl
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/merged/MG-Verilog_high_level_global_summary_in_out_validation.jsonl.idx.info'>
      idx_info_dict = pickle.load(open(idx_fn + ".info", "rb"))
    
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001025
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:165] Computing global indices
[NeMo I 2024-11-27 00:51:26 megatron_gpt_sft_model:840] Length of val dataset: 1573
[NeMo I 2024-11-27 00:51:26 megatron_gpt_sft_model:828] Building GPT SFT test datasets.
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:116] Building data files
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:527] Processing 1 data files using 2 workers
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:493] Building indexing for fn = data/merged/MG-Verilog_high_level_global_summary_in_out_test.jsonl
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:505] Saving idx file = data/merged/MG-Verilog_high_level_global_summary_in_out_test.jsonl.idx.npy
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:507] Saving metadata file = data/merged/MG-Verilog_high_level_global_summary_in_out_test.jsonl.idx.info
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:508: ResourceWarning: unclosed file <_io.BufferedWriter name='data/merged/MG-Verilog_high_level_global_summary_in_out_test.jsonl.idx.info'>
      pickle.dump(data, open(idx_fn + ".info", "wb"))
    
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:542] Time building 1 / 1 mem-mapped files: 0:00:00.097222
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:527] Processing 1 data files using 2 workers
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.086290
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:158] Loading data files
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:249] Loading data/merged/MG-Verilog_high_level_global_summary_in_out_test.jsonl
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/merged/MG-Verilog_high_level_global_summary_in_out_test.jsonl.idx.info'>
      idx_info_dict = pickle.load(open(idx_fn + ".info", "rb"))
    
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000830
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:165] Computing global indices
[NeMo I 2024-11-27 00:51:26 megatron_gpt_sft_model:831] Length of test dataset: 525
[NeMo I 2024-11-27 00:51:26 megatron_gpt_sft_model:847] Building GPT SFT traing datasets.
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:116] Building data files
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:527] Processing 1 data files using 2 workers
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:493] Building indexing for fn = data/merged/MG-Verilog_high_level_global_summary_in_out_train.jsonl
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:505] Saving idx file = data/merged/MG-Verilog_high_level_global_summary_in_out_train.jsonl.idx.npy
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:507] Saving metadata file = data/merged/MG-Verilog_high_level_global_summary_in_out_train.jsonl.idx.info
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:508: ResourceWarning: unclosed file <_io.BufferedWriter name='data/merged/MG-Verilog_high_level_global_summary_in_out_train.jsonl.idx.info'>
      pickle.dump(data, open(idx_fn + ".info", "wb"))
    
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:542] Time building 1 / 1 mem-mapped files: 0:00:00.125021
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:527] Processing 1 data files using 2 workers
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.089722
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:158] Loading data files
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:249] Loading data/merged/MG-Verilog_high_level_global_summary_in_out_train.jsonl
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/merged/MG-Verilog_high_level_global_summary_in_out_train.jsonl.idx.info'>
      idx_info_dict = pickle.load(open(idx_fn + ".info", "rb"))
    
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000867
[NeMo I 2024-11-27 00:51:26 text_memmap_dataset:165] Computing global indices
[NeMo W 2024-11-27 00:51:26 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)
      counts = torch.cuda.LongTensor([1])
    
[NeMo I 2024-11-27 00:51:27 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.06 (sec)
[NeMo I 2024-11-27 00:51:27 megatron_gpt_sft_model:849] Length of train dataset: 644
[NeMo I 2024-11-27 00:51:27 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0
[NeMo I 2024-11-27 00:51:27 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0
[NeMo I 2024-11-27 00:51:27 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0
[NeMo W 2024-11-27 00:51:27 megatron_base_model:1227] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 5.
[NeMo I 2024-11-27 00:51:27 modelPT:787] Optimizer config = MegatronDistributedFusedAdam (
    Parameter Group 0
        betas: [0.9, 0.98]
        bias_correction: True
        eps: 1e-08
        is_expert: False
        lr: 1e-06
        weight_decay: 0.01
    )
[NeMo I 2024-11-27 00:51:27 lr_scheduler:948] Scheduler "<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f9ef81cd060>" 
    will be used during training (effective maximum steps = 5) - 
    Parameters : 
    (warmup_steps: 50
    min_lr: 0.0
    constant_steps: 0
    max_steps: 5
    )
[NeMo W 2024-11-27 00:51:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
    
[NeMo W 2024-11-27 00:51:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.
    
[NeMo I 2024-11-27 00:51:27 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo W 2024-11-27 00:51:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/mappings.py:168: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
      torch.distributed._reduce_scatter_base(
    
[NeMo W 2024-11-27 00:51:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.
      warnings.warn(
    
[NeMo W 2024-11-27 00:51:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:628: UserWarning: When using sequence parallelism it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup
      warnings.warn(
    
[NeMo W 2024-11-27 00:51:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py:78: FutureWarning: `torch.distributed._all_gather_base` is a private function and will be deprecated. Please use `torch.distributed.all_gather_into_tensor` instead.
      return func(*args, **kwargs)
    
[NeMo I 2024-11-27 00:51:57 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo W 2024-11-27 00:51:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2024-11-27 00:51:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2024-11-27 00:51:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
    
[NeMo W 2024-11-27 00:51:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.
    
[NeMo W 2024-11-27 00:51:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.
    
[NeMo W 2024-11-27 00:51:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
    
[NeMo W 2024-11-27 00:51:57 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:475: FutureWarning: `torch.distributed._reduce_scatter_base` is a private function and will be deprecated. Please use `torch.distributed.reduce_scatter_tensor` instead.
      handle = torch.distributed._reduce_scatter_base(
    
[NeMo I 2024-11-27 00:52:37 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo W 2024-11-27 00:54:41 gpt_sft_dataset:331] input is not long enough to truncate.
[NeMo I 2024-11-27 00:55:33 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo W 2024-11-27 00:56:02 nlp_overrides:610] Distributed checkpoints requires DistributedCheckpointIO plugin to be used. Setting up a default now.
[NeMo I 2024-11-27 00:56:02 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.
[NeMo I 2024-11-27 01:00:32 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo W 2024-11-27 01:02:36 gpt_sft_dataset:331] input is not long enough to truncate.
[NeMo I 2024-11-27 01:03:29 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo I 2024-11-27 01:08:23 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo W 2024-11-27 01:10:28 gpt_sft_dataset:331] input is not long enough to truncate.
[NeMo I 2024-11-27 01:11:21 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo I 2024-11-27 01:16:14 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo W 2024-11-27 01:18:18 gpt_sft_dataset:331] input is not long enough to truncate.
[NeMo I 2024-11-27 01:19:11 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo I 2024-11-27 01:24:05 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo W 2024-11-27 01:26:10 gpt_sft_dataset:331] input is not long enough to truncate.
[NeMo I 2024-11-27 01:27:02 num_microbatches_calculator:218] setting number of microbatches to constant 128
[NeMo E 2024-11-27 01:31:22 perf_metrics:84] Failed to calculate TFLOPs per sec per GPU.
    FLOPs measurement not supported for finetuning jobs
[NeMo I 2024-11-27 01:31:22 perf_metrics:86] TFLOPs per sec per GPU=-1.00
[NeMo I 2024-11-27 01:31:22 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.
[NeMo W 2024-11-27 01:32:07 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmptfotd47l'>
      _warnings.warn(warn_message, ResourceWarning)
    
